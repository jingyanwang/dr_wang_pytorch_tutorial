{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e95dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ee6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5b4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8115e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "381d98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import *\n",
    "#TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac0e001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e147be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(\n",
    "    nn.Module\n",
    "    ):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000,\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) \n",
    "            * (-math.log(10000.0)/d_model)\n",
    "            )\n",
    "        \n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        \n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor\n",
    "        ) -> Tensor:\n",
    "        \n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9785eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [7],\n",
       "        [8],\n",
       "        [9]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2317aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PositionalEncoding(\n",
    "    100, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0bbb70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(\n",
    "    nn.Module,\n",
    "    ):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        ntoken: int,\n",
    "        d_model: int,\n",
    "        nhead: int, \n",
    "        d_hid: int,\n",
    "        nlayers: int,\n",
    "        dropout: float = 0.5,\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model, \n",
    "            dropout,)\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            d_model,\n",
    "            nhead,\n",
    "            d_hid,\n",
    "            dropout,)\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            encoder_layers,\n",
    "            nlayers,\n",
    "            )\n",
    "        self.encoder = nn.Embedding(\n",
    "            ntoken,\n",
    "            d_model,\n",
    "            )\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(\n",
    "            d_model,\n",
    "            ntoken,\n",
    "            )\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(\n",
    "        self\n",
    "        ) -> None:\n",
    "        initrage = 0.1\n",
    "        \n",
    "        self.encoder.weight.data.uniform_(\n",
    "            -initrage,\n",
    "            initrage,\n",
    "            )\n",
    "        \n",
    "        self.decoder.bias.data.zero_()\n",
    "        \n",
    "        self.decoder.weight.data.uniform_(\n",
    "            -initrage, \n",
    "            initrage,\n",
    "            )\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        src: Tensor,\n",
    "        src_mask: Tensor,\n",
    "        ) -> Tensor:\n",
    "        \n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate_square_subsequent_mask(\n",
    "        sz: int,\n",
    "        ) -> Tensor:\n",
    "        return torch.triu(\n",
    "            torch.ones(sz, sz) * float('-inf'),\n",
    "            diagonal = 1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12aa57db",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = TransformerModel(\n",
    "    ntoken = 100,\n",
    "    d_model = 100,\n",
    "    nhead = 10, \n",
    "    d_hid = 256,\n",
    "    nlayers = 128,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46086b",
   "metadata": {},
   "source": [
    "# load the batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5fa14dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchdata==0.4.1 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: portalocker>=2.0.0 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata==0.4.1) (2.5.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata==0.4.1) (1.26.11)\n",
      "Requirement already satisfied: torch==1.12.1 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata==0.4.1) (1.12.1+cu116)\n",
      "Requirement already satisfied: requests in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata==0.4.1) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==1.12.1->torchdata==0.4.1) (4.3.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from portalocker>=2.0.0->torchdata==0.4.1) (304)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchdata==0.4.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchdata==0.4.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchdata==0.4.1) (2022.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdata==0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d2da8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading torchtext-0.13.1-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 10.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext) (4.64.0)\n",
      "Requirement already satisfied: torch==1.12.1 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext) (1.12.1+cu116)\n",
      "Requirement already satisfied: requests in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext) (1.23.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==1.12.1->torchtext) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext) (1.26.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\jimwa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->torchtext) (0.4.5)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b14b93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d4fbc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38e67e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a47ef71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9aa9031",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d73f6c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab =  build_vocab_from_iterator(\n",
    "    map(tokenizer, train_iter),\n",
    "    specials = ['<unk>'],                               \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "688b4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a7a016d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1660"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "365afa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(\n",
    "    raw_text_iter: dataset.IterableDataset,\n",
    "    ) -> Tensor:\n",
    "    data = [torch.tensor(\n",
    "        vocab(tokenizer(item))\n",
    "        )\n",
    "        for item in raw_text_iter]\n",
    "    return torch.cat(\n",
    "        tuple(\n",
    "            filter(lambda t: t.numel() > 0, data)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b161d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = WikiText2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c0ad00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_process(\n",
    "    train_iter,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33c14796",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = data_process(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e5d4549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ebfa36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "87baeb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f954c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(\n",
    "    data: Tensor,\n",
    "    bsz: int,\n",
    "    ) -> Tensor:\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cfe9536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 20\n",
    "train_data = batchify(\n",
    "    train_data, \n",
    "    batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5fdf54df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([102499, 20])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "977969ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = batchify(\n",
    "    val_data,\n",
    "    eval_batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "69825bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = batchify(\n",
    "    test_data,\n",
    "    eval_batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47aff77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12092, 20])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8617721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "286163b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(\n",
    "    source: Tensor,\n",
    "    i: int,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "    \n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    \n",
    "    target = source[i+1:i+seq_len].reshape(-1)\n",
    "    \n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d8119775",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = get_batch(test_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ee39defb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 20])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "58e4b83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([680])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7083030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)\n",
    "emsize = 200\n",
    "d_hid = 200\n",
    "nlayers = 2\n",
    "nhead = 2\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "523198b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    emsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    dropout,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "891a6ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b3705310",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8a26b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e680f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr = lr,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7aa09161",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    1.0,\n",
    "    gamma=0.95,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a2728c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module\n",
    "    ) -> None:\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    \n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "432c31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "efb164fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransformerModel.generate_square_subsequent_mask() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [119]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [118]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      8\u001b[0m log_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m      9\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 11\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_square_subsequent_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbptt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: TransformerModel.generate_square_subsequent_mask() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
